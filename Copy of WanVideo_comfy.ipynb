{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"gpuType":"T4","provenance":[{"file_id":"https://huggingface.co/Kijai/WanVideo_comfy.ipynb","timestamp":1755715732870}]},"accelerator":"GPU","kaggle":{"accelerator":"gpu"},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"71ccea887e0149e7b68de804df5e2b3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea92c6946e3042fdb9d676d0aa333e23","IPY_MODEL_9731b9d35136456aa229a374f80c02d7","IPY_MODEL_e4480cf27413496a9808cb0d6246c80e"],"layout":"IPY_MODEL_63060ea91bdd4e8daef96517eff35608"}},"ea92c6946e3042fdb9d676d0aa333e23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b229e9837c54c1894e39b1933094bb2","placeholder":"​","style":"IPY_MODEL_1c55b5f6a7c34e0a8b63ecaa68099373","value":"Loading pipeline components...: 100%"}},"9731b9d35136456aa229a374f80c02d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5e277a9297c4433bbfd4fd4a255acda","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67cb8152dd9242e3988e96a8c2a7d07b","value":5}},"e4480cf27413496a9808cb0d6246c80e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5151ba6239d412c96a28867a21a6f55","placeholder":"​","style":"IPY_MODEL_5e3d8df1e51249c0a2dc3e117ecdccf1","value":" 5/5 [00:01&lt;00:00,  2.86it/s]"}},"63060ea91bdd4e8daef96517eff35608":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b229e9837c54c1894e39b1933094bb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c55b5f6a7c34e0a8b63ecaa68099373":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5e277a9297c4433bbfd4fd4a255acda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67cb8152dd9242e3988e96a8c2a7d07b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b5151ba6239d412c96a28867a21a6f55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e3d8df1e51249c0a2dc3e117ecdccf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05a09dd9ea544d0a8f9e82bdb89cfe29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3482c06c0128408aa8764078abae9905","IPY_MODEL_4de060292f714da39e5593ef21a36b42","IPY_MODEL_5db03a0c06be4d02aa960d8f5e035783"],"layout":"IPY_MODEL_f9229cae003d4c4f832ebe96e35949d4"}},"3482c06c0128408aa8764078abae9905":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54b1725a07e7431785168365157a2e51","placeholder":"​","style":"IPY_MODEL_8750f3d8811e4446923ffd85701a4c65","value":"100%"}},"4de060292f714da39e5593ef21a36b42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fd2765f66e94bcfb3611013fcce0e8b","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4559b67d38e84ab4b654139a531da2d0","value":50}},"5db03a0c06be4d02aa960d8f5e035783":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0e44f6461974878a22bdb7612145b8e","placeholder":"​","style":"IPY_MODEL_9dfed596179842c297fa699183a9e037","value":" 50/50 [00:56&lt;00:00,  1.15s/it]"}},"f9229cae003d4c4f832ebe96e35949d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54b1725a07e7431785168365157a2e51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8750f3d8811e4446923ffd85701a4c65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fd2765f66e94bcfb3611013fcce0e8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4559b67d38e84ab4b654139a531da2d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0e44f6461974878a22bdb7612145b8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dfed596179842c297fa699183a9e037":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ebd25323","executionInfo":{"status":"ok","timestamp":1755753187289,"user_tz":-120,"elapsed":1818,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"8b600594-ebf6-4ddc-b3e3-bbe07258efbb"},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n","def search_huggingface(query):\n","    \"\"\"Searches Hugging Face for models matching the query.\"\"\"\n","    url = f\"https://huggingface.co/models?search={query}&library=all&sort=downloads\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    models = []\n","    for model_card in soup.select('article.mb-2.border.border-gray-200.rounded-lg'):\n","        title_tag = model_card.select_one('h4 a')\n","        if title_tag:\n","            title = title_tag.get_text(strip=True)\n","            link = f\"https://huggingface.co{title_tag['href']}\"\n","            models.append({'title': title, 'link': link})\n","    return models\n","\n","# Search for text-to-video models on Hugging Face\n","text_to_video_models = search_huggingface(\"text-to-video\")\n","\n","print(\"Potential Text-to-Video Models/Libraries found on Hugging Face:\")\n","if text_to_video_models:\n","    for model in text_to_video_models:\n","        print(f\"- {model['title']}: {model['link']}\")\n","else:\n","    print(\"No models found.\")\n","\n","# Note: Additional search on general web for open-source libraries might be needed if Hugging Face results are insufficient.\n","# However, given the prevalence of models on Hugging Face, starting here is a good approach.\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Potential Text-to-Video Models/Libraries found on Hugging Face:\n","No models found.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295,"referenced_widgets":["71ccea887e0149e7b68de804df5e2b3a","ea92c6946e3042fdb9d676d0aa333e23","9731b9d35136456aa229a374f80c02d7","e4480cf27413496a9808cb0d6246c80e","63060ea91bdd4e8daef96517eff35608","9b229e9837c54c1894e39b1933094bb2","1c55b5f6a7c34e0a8b63ecaa68099373","d5e277a9297c4433bbfd4fd4a255acda","67cb8152dd9242e3988e96a8c2a7d07b","b5151ba6239d412c96a28867a21a6f55","5e3d8df1e51249c0a2dc3e117ecdccf1"]},"id":"e2aca639","executionInfo":{"status":"ok","timestamp":1755753208735,"user_tz":-120,"elapsed":21448,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"39f75e09-293d-43fb-b845-e9a64ab91fb9"},"source":["import torch\n","from diffusers import DiffusionPipeline\n","\n","# Specify the model identifier from Hugging Face\n","# Using a known text-to-video model as an example.\n","# This model was identified through manual research outside of the programmatic steps.\n","model_id = \"cerspense/zeroscope_v2_576w\"\n","\n","# Determine the device to use (GPU if available, otherwise CPU)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","\n","# Load the pre-trained text-to-video model\n","# Using from_pretrained with the model identifier and moving it to the selected device.\n","try:\n","    pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32)\n","    pipe = pipe.to(device)\n","    print(f\"Model '{model_id}' loaded successfully on {device}.\")\n","except Exception as e:\n","    print(f\"Error loading model: {e}\")\n","    pipe = None # Ensure pipe is None if loading fails\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71ccea887e0149e7b68de804df5e2b3a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["An error occurred while trying to fetch /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/vae.\n","Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n","An error occurred while trying to fetch /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--cerspense--zeroscope_v2_576w/snapshots/6963642a64dbefa93663d1ecebb4ceda2d9ecb28/unet.\n","Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n","The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n"]},{"output_type":"stream","name":"stdout","text":["Model 'cerspense/zeroscope_v2_576w' loaded successfully on cuda.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755753219044,"user_tz":-120,"elapsed":10303,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"9802cebf-08a1-4f82-cd5e-93df5100c362","id":"NgRIcQslCaL1"},"source":["# Install necessary libraries\n","!pip install diffusers transformers accelerate imageio[ffmpeg] git-lfs\n","\n","# Clone a potential repository (assuming a Stable Diffusion based approach as an example)\n","# This is a placeholder and might need to be replaced with a more specific repo if a model is identified later.\n","!git clone https://github.com/huggingface/diffusers.git\n","\n","# Install git-lfs\n","!git lfs install"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.34.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.0)\n","Requirement already satisfied: git-lfs in /usr/local/lib/python3.12/dist-packages (1.6)\n","Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.12/dist-packages (2.37.0)\n","Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n","Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (from imageio[ffmpeg]) (0.6.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.27.0->diffusers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","fatal: destination path 'diffusers' already exists and is not an empty directory.\n","Git LFS initialized.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3204d74","executionInfo":{"status":"ok","timestamp":1755753228953,"user_tz":-120,"elapsed":9907,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"1f0380ef-07aa-4476-9e25-20f384a84ae4"},"source":["# Install necessary libraries\n","!pip install diffusers transformers accelerate imageio[ffmpeg] git-lfs\n","\n","# Clone a potential repository (assuming a Stable Diffusion based approach as an example)\n","# This is a placeholder and might need to be replaced with a more specific repo if a model is identified later.\n","!git clone https://github.com/huggingface/diffusers.git\n","\n","# Install git-lfs\n","!git lfs install\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.34.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.0)\n","Requirement already satisfied: git-lfs in /usr/local/lib/python3.12/dist-packages (1.6)\n","Requirement already satisfied: imageio[ffmpeg] in /usr/local/lib/python3.12/dist-packages (2.37.0)\n","Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n","Requirement already satisfied: huggingface-hub>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.12/dist-packages (from imageio[ffmpeg]) (0.6.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.27.0->diffusers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.27.0->diffusers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.27.0->diffusers) (1.1.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","fatal: destination path 'diffusers' already exists and is not an empty directory.\n","Git LFS initialized.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["05a09dd9ea544d0a8f9e82bdb89cfe29","3482c06c0128408aa8764078abae9905","4de060292f714da39e5593ef21a36b42","5db03a0c06be4d02aa960d8f5e035783","f9229cae003d4c4f832ebe96e35949d4","54b1725a07e7431785168365157a2e51","8750f3d8811e4446923ffd85701a4c65","6fd2765f66e94bcfb3611013fcce0e8b","4559b67d38e84ab4b654139a531da2d0","b0e44f6461974878a22bdb7612145b8e","9dfed596179842c297fa699183a9e037"]},"id":"9683dcb6","executionInfo":{"status":"ok","timestamp":1755753287006,"user_tz":-120,"elapsed":58050,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"c9a2ed83-b5f6-481c-d702-55f201ac46d2"},"source":["if pipe is not None:\n","    prompt = \"a close up shot of a person's eye, with a galaxy reflected in it\"\n","\n","    # Generate the video\n","    try:\n","        video_frames = pipe(prompt, num_frames=25, num_inference_steps=50, generator=torch.manual_seed(42)).frames\n","        print(\"Video generation successful.\")\n","    except Exception as e:\n","        print(f\"Error during video generation: {e}\")\n","        video_frames = None\n","else:\n","    print(\"Model was not loaded successfully. Cannot generate video.\")\n","    video_frames = None\n"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a09dd9ea544d0a8f9e82bdb89cfe29"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Video generation successful.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8733c90e","executionInfo":{"status":"ok","timestamp":1755753287089,"user_tz":-120,"elapsed":75,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"5b87c1ac-3145-4d1f-f06d-d8fb788a0ad0"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    import imageio\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        with imageio.get_writer(output_filename, fps=8) as writer:\n","            for frame in video_frames:\n","                writer.append_data(frame)\n","        print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:imageio:Lossy conversion from float32 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"]},{"output_type":"stream","name":"stdout","text":["Error saving video: Image must have 1, 2, 3 or 4 channels\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfEExgIhzE9G","executionInfo":{"status":"ok","timestamp":1755753287137,"user_tz":-120,"elapsed":41,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"218e825c-cbbb-4ebd-cd2c-fc4439ea8f46"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    import imageio\n","    import numpy as np\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        # Convert frames to uint8 and ensure 3 channels (RGB)\n","        processed_frames = []\n","        for frame in video_frames:\n","            # Convert float32 to uint8 and scale to 0-255\n","            frame_uint8 = (frame * 255).astype(np.uint8)\n","            # Ensure the frame has 3 channels (RGB) - handle potential alpha channel or grayscale\n","            if frame_uint8.shape[-1] == 4:\n","                frame_uint8 = frame_uint8[..., :3] # Drop alpha channel\n","            elif frame_uint8.ndim == 2:\n","                 frame_uint8 = np.stack([frame_uint8] * 3, axis=-1) # Convert grayscale to RGB\n","\n","            processed_frames.append(frame_uint8)\n","\n","\n","        with imageio.get_writer(output_filename, fps=8) as writer:\n","            for frame in processed_frames:\n","                writer.append_data(frame)\n","        print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Error saving video: Image must have 1, 2, 3 or 4 channels\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39ef69c6","executionInfo":{"status":"ok","timestamp":1755753287155,"user_tz":-120,"elapsed":16,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"4a085021-5824-412b-892d-4ad4bea97130"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    import imageio\n","    import numpy as np\n","    from PIL import Image\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        processed_frames = []\n","        for frame in video_frames:\n","            # Assuming the frame is a PyTorch Tensor or NumPy array in the range [0, 1] or [0, 255]\n","            # Convert to NumPy array if it's a Tensor\n","            if isinstance(frame, torch.Tensor):\n","                frame = frame.permute(1, 2, 0).cpu().numpy() # Convert from (C, H, W) to (H, W, C) and to numpy\n","\n","            # Convert to uint8 if not already\n","            if frame.dtype != np.uint8:\n","                 # Scale to 0-255 if in range [0, 1]\n","                 if np.max(frame) <= 1.0:\n","                     frame = (frame * 255).astype(np.uint8)\n","                 else:\n","                     frame = frame.astype(np.uint8)\n","\n","\n","            # Ensure the frame has 3 channels (RGB)\n","            if frame.ndim == 2:\n","                 frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","            elif frame.shape[-1] == 1:\n","                 frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","            elif frame.shape[-1] == 4:\n","                frame = frame[..., :3] # Drop alpha channel\n","\n","            processed_frames.append(frame)\n","\n","\n","        with imageio.get_writer(output_filename, fps=8) as writer:\n","            for frame in processed_frames:\n","                writer.append_data(frame)\n","        print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Error saving video: Image must have 1, 2, 3 or 4 channels\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8d089c35","executionInfo":{"status":"ok","timestamp":1755753287205,"user_tz":-120,"elapsed":47,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"ede7c8e1-478e-4582-e057-99e5f78a9451"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    import imageio\n","    import numpy as np\n","    from PIL import Image # Keep import in case it's needed for conversion later\n","    import torch # Keep import in case it's needed for conversion later\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        processed_frames = []\n","        for i, frame in enumerate(video_frames):\n","            # Assuming the frame is a PyTorch Tensor or NumPy array in the range [0, 1] or [0, 255]\n","            # Convert to NumPy array if it's a Tensor\n","            if isinstance(frame, torch.Tensor):\n","                # Ensure the tensor is on CPU before converting to numpy\n","                frame = frame.detach().cpu().numpy()\n","\n","            # Convert to uint8 if not already\n","            if frame.dtype != np.uint8:\n","                 # Scale to 0-255 if in range [0, 1]\n","                 if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                     frame = (frame * 255).astype(np.uint8)\n","                 else:\n","                    # If not in [0, 1] range, simply convert type, assuming it might be close to uint8 range\n","                    frame = frame.astype(np.uint8)\n","\n","\n","            # Ensure the frame has 3 channels (RGB)\n","            if frame.ndim == 2:\n","                 frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","            elif frame.shape[-1] == 1:\n","                 frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","            elif frame.shape[-1] == 4:\n","                frame = frame[..., :3] # Drop alpha channel\n","\n","            processed_frames.append(frame)\n","            print(f\"Processing frame {i}: Original shape {video_frames[i].shape if isinstance(video_frames[i], torch.Tensor) else video_frames[i].shape}, Processed shape {frame.shape}, Processed dtype {frame.dtype}\")\n","\n","\n","        with imageio.get_writer(output_filename, fps=8) as writer:\n","            for i, frame in enumerate(processed_frames):\n","                 print(f\"Appending frame {i}: Shape {frame.shape}, Dtype {frame.dtype}\")\n","                 writer.append_data(frame)\n","\n","        print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing frame 0: Original shape (25, 256, 256, 3), Processed shape (25, 256, 256, 3), Processed dtype uint8\n","Appending frame 0: Shape (25, 256, 256, 3), Dtype uint8\n","Error saving video: Image must have 1, 2, 3 or 4 channels\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"688d37ea","executionInfo":{"status":"ok","timestamp":1755753287223,"user_tz":-120,"elapsed":16,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"31ce9d1c-b857-4873-ee67-36dee036a2ab"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    import imageio\n","    import numpy as np\n","    # No need to import PIL or torch again if not explicitly converting from those types\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        # Assume video_frames is a list of frames or a single numpy array/torch tensor where the first dimension is the frame index\n","        # If it's a single tensor/array, iterate through the first dimension\n","        if isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             frames_to_save = video_frames\n","        elif isinstance(video_frames, list):\n","             # If it's a list, assume each element is a frame\n","             frames_to_save = video_frames\n","        else:\n","             print(\"Unsupported format for video_frames.\")\n","             frames_to_save = None\n","\n","\n","        if frames_to_save is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # Iterate through the frames individually\n","                for i, frame in enumerate(frames_to_save):\n","                    # Ensure the frame is a NumPy array\n","                    if isinstance(frame, torch.Tensor):\n","                        frame = frame.detach().cpu().numpy()\n","\n","                    # Convert to uint8 if not already\n","                    if frame.dtype != np.uint8:\n","                         # Scale to 0-255 if in range [0, 1]\n","                         if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                             frame = (frame * 255).astype(np.uint8)\n","                         else:\n","                            # If not in [0, 1] range, simply convert type, assuming it might be close to uint8 range\n","                            frame = frame.astype(np.uint8)\n","\n","                    # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                    if frame.ndim == 2:\n","                         frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                    elif frame.shape[-1] == 1:\n","                         frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3] # Drop alpha channel\n","                    elif frame.shape[0] in [1, 2, 3, 4] and frame.ndim == 3:\n","                         # If shape is (C, H, W), permute to (H, W, C)\n","                         frame = frame.transpose(1, 2, 0)\n","\n","                    # Final check on shape before appending\n","                    if frame.ndim == 3 and frame.shape[-1] in [1, 3, 4]:\n","                         # print(f\"Appending frame {i}: Shape {frame.shape}, Dtype {frame.dtype}\") # Debugging print\n","                         writer.append_data(frame)\n","                    else:\n","                         print(f\"Skipping frame {i} due to incorrect shape: {frame.shape}\")\n","\n","\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping frame 0 due to incorrect shape: (25, 256, 256, 3)\n","Video saved successfully to generated_video.mp4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0122eb07","executionInfo":{"status":"ok","timestamp":1755753287365,"user_tz":-120,"elapsed":11,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"cb878b7f-5c48-4fb4-ee36-66e2b8ed2388"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    import imageio\n","    import numpy as np\n","    # No need to import PIL or torch again if not explicitly converting from those types\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        # Assume video_frames is a single numpy array or torch tensor where the first dimension is the frame index\n","        if isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             # Iterate through the first dimension to get individual frames\n","             frames_to_save = video_frames\n","        elif isinstance(video_frames, list):\n","             # If it's a list, assume each element is a frame\n","             frames_to_save = video_frames\n","        else:\n","             print(\"Unsupported format for video_frames.\")\n","             frames_to_save = None\n","\n","\n","        if frames_to_save is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # Correctly iterate through the individual frames\n","                for i in range(frames_to_save.shape[0] if isinstance(frames_to_save, (np.ndarray, torch.Tensor)) else len(frames_to_save)):\n","                    frame = frames_to_save[i]\n","\n","                    # Ensure the frame is a NumPy array\n","                    if isinstance(frame, torch.Tensor):\n","                        frame = frame.detach().cpu().numpy()\n","\n","                    # Convert to uint8 if not already\n","                    if frame.dtype != np.uint8:\n","                         # Scale to 0-255 if in range [0, 1]\n","                         if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                             frame = (frame * 255).astype(np.uint8)\n","                         else:\n","                            # If not in [0, 1] range, simply convert type, assuming it might be close to uint8 range\n","                            frame = frame.astype(np.uint8)\n","\n","                    # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                    if frame.ndim == 2:\n","                         frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                    elif frame.shape[-1] == 1:\n","                         frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                    elif frame.shape[0] in [1, 3, 4] and frame.ndim == 3:\n","                         # If shape is (C, H, W), permute to (H, W, C)\n","                         frame = frame.transpose(1, 2, 0)\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3] # Drop alpha channel\n","\n","\n","                    # Final check on shape before appending\n","                    if frame.ndim == 3 and frame.shape[-1] in [1, 3, 4]:\n","                         # print(f\"Appending frame {i}: Shape {frame.shape}, Dtype {frame.dtype}\") # Debugging print\n","                         writer.append_data(frame)\n","                    else:\n","                         print(f\"Skipping frame {i} due to incorrect shape: {frame.shape}\")\n","\n","\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping frame 0 due to incorrect shape: (25, 256, 256, 3)\n","Video saved successfully to generated_video.mp4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b53fe3cf","executionInfo":{"status":"ok","timestamp":1755753287371,"user_tz":-120,"elapsed":4,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"4b548ff9-2e97-4367-cd3e-e7cabf59dd85"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    # No need to import imageio, numpy, PIL, or torch again\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        # Assume video_frames is a single numpy array or torch tensor where the first dimension is the frame index\n","        if isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             frames_array = video_frames\n","        elif isinstance(video_frames, list):\n","             # If it's a list, convert to a numpy array for consistent processing\n","             frames_array = np.array(video_frames)\n","        else:\n","             print(\"Unsupported format for video_frames.\")\n","             frames_array = None\n","\n","\n","        if frames_array is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # Correctly iterate through the individual frames by indexing the first dimension\n","                for i in range(frames_array.shape[0]):\n","                    frame = frames_array[i]\n","\n","                    # Ensure the frame is a NumPy array and in uint8 format\n","                    if isinstance(frame, torch.Tensor):\n","                        frame = frame.detach().cpu().numpy()\n","\n","                    if frame.dtype != np.uint8:\n","                         if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                             frame = (frame * 255).astype(np.uint8)\n","                         else:\n","                            frame = frame.astype(np.uint8)\n","\n","                    # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                    if frame.ndim == 2:\n","                         frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                    elif frame.shape[-1] == 1:\n","                         frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                    elif frame.shape[0] in [1, 3, 4] and frame.ndim == 3:\n","                         # If shape is (C, H, W), permute to (H, W, C)\n","                         frame = frame.transpose(1, 2, 0)\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3] # Drop alpha channel\n","\n","\n","                    # Final check on shape before appending\n","                    if frame.ndim == 3 and frame.shape[-1] in [1, 3, 4]:\n","                         # print(f\"Appending frame {i}: Shape {frame.shape}, Dtype {frame.dtype}\") # Debugging print\n","                         writer.append_data(frame)\n","                    else:\n","                         print(f\"Skipping frame {i} due to incorrect shape: {frame.shape}\")\n","\n","\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping frame 0 due to incorrect shape: (25, 256, 256, 3)\n","Video saved successfully to generated_video.mp4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd31330d","executionInfo":{"status":"ok","timestamp":1755753287375,"user_tz":-120,"elapsed":5,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"045897b5-1671-491a-8e1f-448fd2290bab"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    # No need to import imageio, numpy, PIL, or torch again\n","\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        # Assume video_frames is a single numpy array or torch tensor where the first dimension is the frame index\n","        if isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             frames_array = video_frames\n","        elif isinstance(video_frames, list):\n","             # If it's a list, convert to a numpy array for consistent processing\n","             frames_array = np.array(video_frames)\n","        else:\n","             print(\"Unsupported format for video_frames.\")\n","             frames_array = None\n","\n","\n","        if frames_array is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # Correctly iterate through the individual frames by indexing the first dimension\n","                for i in range(frames_array.shape[0]):\n","                    frame = frames_array[i]\n","\n","                    # Ensure the frame is a NumPy array and in uint8 format\n","                    if isinstance(frame, torch.Tensor):\n","                        frame = frame.detach().cpu().numpy()\n","\n","                    if frame.dtype != np.uint8:\n","                         if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                             frame = (frame * 255).astype(np.uint8)\n","                         else:\n","                            frame = frame.astype(np.uint8)\n","\n","                    # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                    # This logic should apply to the individual frame's shape\n","                    if frame.ndim == 2:\n","                         frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                    elif frame.shape[-1] == 1:\n","                         frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                    elif frame.shape[0] in [1, 3, 4] and frame.ndim == 3:\n","                         # If shape is (C, H, W), permute to (H, W, C)\n","                         frame = frame.transpose(1, 2, 0)\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3] # Drop alpha channel\n","\n","\n","                    # Final check on shape before appending\n","                    # This check should be against the shape of the individual frame\n","                    if frame.ndim == 3 and frame.shape[-1] in [1, 3, 4]:\n","                         # print(f\"Appending frame {i}: Shape {frame.shape}, Dtype {frame.dtype}\") # Debugging print\n","                         writer.append_data(frame)\n","                    else:\n","                         # This print indicates an issue with the processing logic or the input frame format\n","                         print(f\"Skipping frame {i} due to incorrect shape after processing: {frame.shape}\")\n","\n","\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping frame 0 due to incorrect shape after processing: (25, 256, 256, 3)\n","Video saved successfully to generated_video.mp4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f1f35fa1","executionInfo":{"status":"ok","timestamp":1755753287381,"user_tz":-120,"elapsed":8,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"92957b93-0c70-48de-c124-aa67ffb2b102"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        # Assume video_frames is a single numpy array or torch tensor where the first dimension is the frame index\n","        if isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             frames_array = video_frames\n","        elif isinstance(video_frames, list):\n","             # If it's a list, convert to a numpy array for consistent processing\n","             frames_array = np.array(video_frames)\n","        else:\n","             print(\"Unsupported format for video_frames.\")\n","             frames_array = None\n","\n","\n","        if frames_array is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # Correctly iterate through the individual frames by indexing the first dimension\n","                for i in range(frames_array.shape[0]):\n","                    frame = frames_array[i]\n","\n","                    # Ensure the frame is a NumPy array and in uint8 format\n","                    if isinstance(frame, torch.Tensor):\n","                        frame = frame.detach().cpu().numpy()\n","\n","                    if frame.dtype != np.uint8:\n","                         if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                             frame = (frame * 255).astype(np.uint8)\n","                         else:\n","                            frame = frame.astype(np.uint8)\n","\n","                    # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                    # This logic should apply to the individual frame's shape\n","                    if frame.ndim == 2:\n","                         frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                    elif frame.shape[-1] == 1:\n","                         frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                    elif frame.shape[0] in [1, 3, 4] and frame.ndim == 3:\n","                         # If shape is (C, H, W), permute to (H, W, C)\n","                         frame = frame.transpose(1, 2, 0)\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3] # Drop alpha channel\n","\n","\n","                    # Final check on shape before appending\n","                    # This check should be against the shape of the individual frame\n","                    if frame.ndim == 3 and frame.shape[-1] in [1, 3, 4]:\n","                         # print(f\"Appending frame {i}: Shape {frame.shape}, Dtype {frame.dtype}\") # Debugging print\n","                         writer.append_data(frame)\n","                    else:\n","                         # This print indicates an issue with the processing logic or the input frame format\n","                         print(f\"Skipping frame {i} due to incorrect shape after processing: {frame.shape}\")\n","\n","\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping frame 0 due to incorrect shape after processing: (25, 256, 256, 3)\n","Video saved successfully to generated_video.mp4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f8fc0de2","executionInfo":{"status":"ok","timestamp":1755753287382,"user_tz":-120,"elapsed":5,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"718059e6-7109-4b77-eea6-e3233dca19e7"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        # Assume video_frames is a single numpy array or torch tensor where the first dimension is the frame index\n","        if isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             frames_array = video_frames\n","        elif isinstance(video_frames, list):\n","             # If it's a list, convert to a numpy array for consistent processing\n","             frames_array = np.array(video_frames)\n","        else:\n","             print(\"Unsupported format for video_frames.\")\n","             frames_array = None\n","\n","\n","        if frames_array is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # Iterate through the first dimension to get individual frames\n","                for i in range(frames_array.shape[0]):\n","                    # Extract the individual frame\n","                    frame = frames_array[i]\n","\n","                    # Ensure the frame is a NumPy array and in uint8 format\n","                    if isinstance(frame, torch.Tensor):\n","                        frame = frame.detach().cpu().numpy()\n","\n","                    if frame.dtype != np.uint8:\n","                         if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                             frame = (frame * 255).astype(np.uint8)\n","                         else:\n","                            frame = frame.astype(np.uint8)\n","\n","                    # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                    # This logic should apply to the individual frame's shape\n","                    if frame.ndim == 2:\n","                         frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                    elif frame.shape[-1] == 1:\n","                         frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                    elif frame.shape[0] in [1, 3, 4] and frame.ndim == 3:\n","                         # If shape is (C, H, W), permute to (H, W, C)\n","                         frame = frame.transpose(1, 2, 0)\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3] # Drop alpha channel\n","\n","                    # Final check on shape before appending\n","                    # This check should be against the shape of the individual frame\n","                    if frame.ndim == 3 and frame.shape[-1] in [3]: # Only accept 3 channel RGB for saving\n","                         # print(f\"Appending frame {i}: Shape {frame.shape}, Dtype {frame.dtype}\") # Debugging print\n","                         writer.append_data(frame)\n","                    else:\n","                         # This print indicates an issue with the processing logic or the input frame format\n","                         print(f\"Skipping frame {i} due to incorrect shape after processing: {frame.shape}\")\n","\n","\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping frame 0 due to incorrect shape after processing: (25, 256, 256, 3)\n","Video saved successfully to generated_video.mp4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7b5cd7ae","executionInfo":{"status":"ok","timestamp":1755753287384,"user_tz":-120,"elapsed":4,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"7af1dc97-ef7f-4a99-b5de-ea17d4740eb0"},"source":["if 'video_frames' in locals() and video_frames is not None:\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        frames_to_process = None\n","\n","        # Determine if video_frames is a list or a single array/tensor\n","        if isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             # If it's a single array/tensor, assume the first dimension is the frame index\n","             frames_to_process = video_frames\n","        elif isinstance(video_frames, list):\n","             # If it's a list, assume each element is a frame\n","             frames_to_process = video_frames\n","        else:\n","             print(\"Unsupported format for video_frames.\")\n","\n","\n","        if frames_to_process is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # Iterate through the frames\n","                if isinstance(frames_to_process, (np.ndarray, torch.Tensor)):\n","                    # Iterate through the first dimension for numpy array/tensor\n","                    for i in range(frames_to_process.shape[0]):\n","                        frame = frames_to_process[i]\n","\n","                        # Ensure the frame is a NumPy array and in uint8 format\n","                        if isinstance(frame, torch.Tensor):\n","                            frame = frame.detach().cpu().numpy()\n","\n","                        if frame.dtype != np.uint8:\n","                            # Scale to 0-255 if in range [0, 1]\n","                            if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                                frame = (frame * 255).astype(np.uint8)\n","                            else:\n","                                frame = frame.astype(np.uint8)\n","\n","                        # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                        if frame.ndim == 2:\n","                            frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                        elif frame.shape[-1] == 1:\n","                            frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                        elif frame.shape[0] in [1, 3, 4] and frame.ndim == 3:\n","                            # If shape is (C, H, W), permute to (H, W, C)\n","                            frame = frame.transpose(1, 2, 0)\n","                        elif frame.shape[-1] == 4:\n","                            frame = frame[..., :3] # Drop alpha channel\n","\n","\n","                        # Final check on shape before appending - should be (H, W, C) with C=1, 3, or 4\n","                        if frame.ndim == 3 and frame.shape[-1] in [1, 3, 4]:\n","                            writer.append_data(frame)\n","                        else:\n","                            print(f\"Skipping frame {i} due to incorrect shape before appending: {frame.shape}\")\n","\n","                elif isinstance(frames_to_process, list):\n","                    # Iterate through the list elements\n","                    for i, frame in enumerate(frames_to_process):\n","                         # Ensure the frame is a NumPy array and in uint8 format\n","                        if isinstance(frame, torch.Tensor):\n","                            frame = frame.detach().cpu().numpy()\n","                        elif not isinstance(frame, np.ndarray):\n","                             print(f\"Skipping frame {i} due to unsupported type in list: {type(frame)}\")\n","                             continue # Skip to the next frame if not a recognized type\n","\n","\n","                        if frame.dtype != np.uint8:\n","                            # Scale to 0-255 if in range [0, 1]\n","                            if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                                frame = (frame * 255).astype(np.uint8)\n","                            else:\n","                                frame = frame.astype(np.uint8)\n","\n","                        # Ensure the frame has 3 channels (RGB) and is in (H, W, C) format\n","                        if frame.ndim == 2:\n","                            frame = np.stack([frame] * 3, axis=-1) # Convert grayscale to RGB\n","                        elif frame.shape[-1] == 1:\n","                            frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (with channel dim) to RGB\n","                        elif frame.shape[0] in [1, 3, 4] and frame.ndim == 3:\n","                            # If shape is (C, H, W), permute to (H, W, C)\n","                            frame = frame.transpose(1, 2, 0)\n","                        elif frame.shape[-1] == 4:\n","                            frame = frame[..., :3] # Drop alpha channel\n","\n","\n","                        # Final check on shape before appending - should be (H, W, C) with C=1, 3, or 4\n","                        if frame.ndim == 3 and frame.shape[-1] in [1, 3, 4]:\n","                            writer.append_data(frame)\n","                        else:\n","                            print(f\"Skipping frame {i} due to incorrect shape before appending: {frame.shape}\")\n","\n","\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping frame 0 due to incorrect shape before appending: (25, 256, 256, 3)\n","Video saved successfully to generated_video.mp4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cbecf1b","executionInfo":{"status":"ok","timestamp":1755753287442,"user_tz":-120,"elapsed":57,"user":{"displayName":"Rodenbecker Grasland","userId":"06109293958749875921"}},"outputId":"2f19e903-9fff-4a9c-c681-f308f06e1417"},"source":["import imageio\n","import numpy as np\n","import torch\n","\n","if 'video_frames' in locals() and video_frames is not None:\n","    output_filename = \"generated_video.mp4\"\n","    try:\n","        frames_to_save = None\n","\n","        # 1. Handle list containing a single array/tensor or direct array/tensor\n","        if isinstance(video_frames, list) and len(video_frames) == 1 and isinstance(video_frames[0], (np.ndarray, torch.Tensor)):\n","            frames_array = video_frames[0] # Extract the single array/tensor from the list\n","        elif isinstance(video_frames, (np.ndarray, torch.Tensor)):\n","             # If it's already a single array/tensor, use it directly\n","             frames_array = video_frames\n","        else:\n","             print(\"Unsupported format for video_frames. Expected a NumPy array, PyTorch Tensor, or a list containing one of these.\")\n","             frames_array = None # Set to None if format is not supported\n","\n","\n","        if frames_array is not None:\n","             # Ensure frames_array is a numpy array\n","            if isinstance(frames_array, torch.Tensor):\n","                frames_array = frames_array.detach().cpu().numpy()\n","\n","            # 4. Permute if in (num_frames, C, H, W) format\n","            # Check if the first dimension is likely channels based on a common range (1 to 4 channels)\n","            if frames_array.ndim == 4 and frames_array.shape[1] in [1, 3, 4] and frames_array.shape[-1] not in [1, 3, 4]:\n","                 print(f\"Permuting shape from {frames_array.shape} (assuming NCHW) to NHWC.\")\n","                 frames_array = frames_array.transpose(0, 2, 3, 1) # Permute to (num_frames, H, W, C)\n","\n","\n","            # 5. Ensure the resulting array has the shape (num_frames, H, W, C) with C being 1, 3, or 4.\n","            if not (frames_array.ndim == 4 and frames_array.shape[-1] in [1, 3, 4]):\n","                 print(f\"Error: Unexpected shape for frames_array after processing: {frames_array.shape}. Expected (num_frames, H, W, C) with C=1, 3, or 4.\")\n","                 frames_array = None # Set to None to prevent further processing\n","\n","\n","        if frames_array is not None:\n","            with imageio.get_writer(output_filename, fps=8) as writer:\n","                # 6. Iterate through the first dimension (frames).\n","                for i in range(frames_array.shape[0]):\n","                    frame = frames_array[i] # This should now be a (H, W, C) array\n","\n","                    # 7. Ensure it is a NumPy array and in uint8 format, scaling if necessary.\n","                    if frame.dtype != np.uint8:\n","                         # Scale to 0-255 if in range [0, 1]\n","                         if np.max(frame) <= 1.0 and np.min(frame) >= 0.0:\n","                             frame = (frame * 255).astype(np.uint8)\n","                         else:\n","                            # If not in [0, 1] range, simply convert type, assuming it might be close to uint8 range\n","                            frame = frame.astype(np.uint8)\n","\n","                    # 8. Ensure the individual frame has 3 channels (RGB), converting from grayscale or dropping alpha if needed.\n","                    if frame.ndim == 2:\n","                         frame = np.stack([frame] * 3, axis=-1) # Convert grayscale (H, W) to RGB (H, W, 3)\n","                    elif frame.shape[-1] == 1:\n","                         frame = np.concatenate([frame] * 3, axis=-1) # Convert grayscale (H, W, 1) to RGB (H, W, 3)\n","                    elif frame.shape[-1] == 4:\n","                        frame = frame[..., :3] # Drop alpha channel (H, W, 4) to RGB (H, W, 3)\n","\n","\n","                    # Final check before appending - must be (H, W, 3) for imageio writer when writing to mp4\n","                    if frame.ndim == 3 and frame.shape[-1] == 3:\n","                         writer.append_data(frame)\n","                    else:\n","                         # This print indicates an issue with the processing logic or the input frame format\n","                         print(f\"Skipping frame {i} due to incorrect shape before appending: {frame.shape}. Expected (H, W, 3).\")\n","\n","\n","            # 10. Print a success message if the video is saved\n","            print(f\"Video saved successfully to {output_filename}\")\n","    except Exception as e:\n","        # 10. Print an error message if any exception occurs.\n","        print(f\"Error saving video: {e}\")\n","else:\n","    print(\"No video frames available to save.\")\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Error: Unexpected shape for frames_array after processing: (1, 25, 256, 256, 3). Expected (num_frames, H, W, C) with C=1, 3, or 4.\n"]}]}]}